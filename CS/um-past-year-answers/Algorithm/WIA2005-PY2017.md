# Part A
---
## Question 1
$O(n)$

## Question 2
The best algorithm for sorting an array that contains only the elements -1, 0, and 1 is **Counting Sort**. 

**Algorithm:**
1. **Count the occurrences** of -1, 0, and 1.
2. **Reconstruct the array** using these counts.

**Running Time:**
- The time complexity is $O(n)$, where $n$ is the number of elements in the array.

**Example Pseudocode:**
```pseudo
FUNCTION COUNTINGSORT(A):
    COUNT[-1] = 0
    COUNT[0] = 0
    COUNT[1] = 0
    
    FOR EACH element IN A:
        INCREMENT COUNT[element]
    
    INDEX = 0
    FOR EACH value IN [-1, 0, 1]:
        FOR i FROM 1 TO COUNT[value]:
            A[INDEX] = value
            INDEX = INDEX + 1
```

This approach is optimal because it leverages the limited range of possible values to achieve linear time complexity.

## Question 3
### 3a)
- Quicksort's worst-case time complexity is $O(n^2)$, occurring when the pivot selection results in highly unbalanced partitions, such as when the input array is already sorted. 
  
- For instance, in `[1, 2, 3, 1, 5, 6, 7, 8, 9, 10]`, selecting the last element as the pivot results in partitions of size `n-1` and `1` at each step. 
  
- The total number of comparisons can be represented by the sum: $n + (n - 1) + (n - 2) + \ldots + 1$
  
  Summing these terms gives:   $\frac{n(n + 1)}{2}$ 
  
  Resulting in a time complexity of $O(n^2)$.

### 3b)

In the best-case scenario, Quicksort partitions the array into two equal halves at each step. The recurrence relation for this case is: $T(n) = 2T\left(\frac{n}{2}\right) + n$

Using the recurrence tree method, each level of the tree has a total cost of $n$, and there are $\log_2 n$ levels. Thus, the total time complexity is: $T(n) = n \log_2 n$. Therefore, the time complexity is $O(n \log n)$.

### 3c)
1. **Calculate the midpoint**: Determine the midpoint of the array or subarray.
2. **Check the midpoint value**: Compare the value at the midpoint with its expected value.
   - If `A[m] != m + 1`, it means the missing number is either at the midpoint or in the left subarray:
     - If the midpoint is the first element or the element before the midpoint is in the correct position, then the missing number is `m + 1`.
     - Otherwise, search in the left subarray.
   - If `A[m] == m + 1`, continue the search in the right subarray.

**Pseudocode**:
```pseufo
FINDMISSINGNUMBER(A, i, j):
    m = i + (j - i) // 2
    if A[m] != m + 1:
        if m == 0 or A[m-1] == m:
            return m + 1
        return FINDMISSINGNUMBER(A, i, m - 1)
    return FINDMISSINGNUMBER(A, m + 1, j)
```

This algorithm efficiently narrows down the search space to find the missing number in logarithmic time, $O(\log n)$.

**Additional Notes:**

- The numbers are given serially, starting from `001, 002, ...`.
- There is only one missing number, and the last running number is `998`.

If there were more missing numbers or other complexities, a different approach like linear searching would be more suitable. This would involve initializing an array of size `999`, iterating through each element to mark the presence of numbers, and then finding the index with the `false` value.


# Part B
---
## Question 1
Given an array of digits (values are from 0 to 9), find the minimum possible sum of two numbers formed from digits of the array. All digits of the given array must be used to form the two numbers.

### Naive Approach
The naive approach involves generating all possible combinations of the digits to form two numbers and finding the combination that gives the minimum sum. This is computationally expensive due to the factorial complexity.

$T(n)$:
$$\frac{n!}{\left(n-\lfloor(\frac{n}{2})\rfloor\right)!}+(\frac{n}{2}!)$$
Hence, its time complexity will be $O(n!)$.

**Pseudocode:**
```pseudo
FINDMINSUM_NAIVE(A):
    MIN_SUM = INFINITY
    FOR each possible way to split A into two parts:
        NUM1, NUM2 = form two numbers from the two parts
        SUM = NUM1 + NUM2
        IF SUM < MIN_SUM:
            MIN_SUM = SUM
    RETURN MIN_SUM
```

**Implementation:**
```pseudo
FINDMINSUM(A):
    COMBINATIONS = FINDCOMBINATIONS([], A, 0, 1)
    SORT(COMBINATIONS)
    RETURN COMBINATIONS[0] + COMBINATIONS[1]

FINDCOMBINATIONS(RESULTS, A, sum, j):
    IF LENGTH(A) == 0:
        APPEND(RESULTS, sum)
        RETURN
    ENDIF
    FOR i FROM 0 TO LENGTH(A) - 1:
        new_sum = sum + A[i] * 10 ^ ((j - 1) % 2 + (j - 1) // 2)
        FINDCOMBINATIONS(RESULTS, A[0:i] + A[i+1:], new_sum, j + 1)
    ENDFOR
```

**Time Complexity:** $O(n!)$

### Optimized Approach
A more efficient approach is to sort the array and then sum the digits from the last element, multiplying them by a constant.

**Example 1**:
Given an array $[6, 8, 4, 5, 2, 3]$, we can form the minimum sum by assigning the two largest values to the units place:
- $c_1 = 8, c_2 = 6$

Next, we assign the 3rd and 4th largest values to the tens place:
- $c_1 = 58, c_2 = 46$

Finally, we assign the smallest values to the hundreds place:
- $c_1 = 358, c_2 = 246$

Therefore, we follow these steps:
1. **Sort the array** to be $[2, 3, 4, 5, 6, 8]$.
2. **Sum the values from the last element, multiplying them by a constant.**

How do we determine the constant? Let's take a closer look at how we assign the digits to their positions:

- $n$ is the length of the array, $A.length$.
- $i$ is the index of the value.
- $v$ is the value.
- $c$ is the constant.

For example:
1. $n = 6, i = 5, v = 8, c = 1$
2. $n = 6, i = 4, v = 6, c = 1$
3. $n = 6, i = 3, v = 5, c = 10$
1. $n = 6, i = 2, v = 1, c = 10$

We observe the pattern:
1. For $n = 6, i = 5, v = 8, c = 1$, $n - i = 1$
2. For $n = 6, i = 1, v = 6, c = 1$, $n - i = 2$

To keep the result within a range, we use the following formula to calculate the constant correctly:
$(k\ \text{mod}\ 2) + \frac{k}{2} -1$
This formula ensures the correct positional value for each digit.

**Time Complexity**
- Sorting the array: $O(n \log n)$
- Summing the values: $O(n)$
- Overall time complexity: $O(n \log n)$

Pseudocode:
```pseudo
FUNCTION FINDMINSUM(A):
	SORT(A)
	SUM = 0
	FOR i FROM LENGTH(A) - 1 TO 0 STEP -1:
		K = A.length - i
		SUM += A[i] * 10 ^ ((K & 1) + (K // 2))
	RETURN SUM
```

**Implementation**:
```python
def find_min_sum_iterative(arr):
    arr.sort()
    sum = 0
    for i in range(len(arr) - 1, -1, -1):
        k = len(arr) - i
        sum += arr[i] * 10 ** (k % 2 + k // 2 - 1)
    return sum
```
or
```python
def find_min_sum_naive(arr):
    num1 = []
    num2 = []
    def find_min_sum_recursive(arr, num1, num2):
        if not arr:
            return (num1, num2)
        if len(num1) == len(num2):
            num1.append(arr.pop(0))
        else:
            num2.append(arr.pop(0))
        return find_min_sum_recursive(arr, num1, num2)
    arr.sort()
    return sum(int(''.join(map(str, num))) for num in find_min_sum_recursive(arr, num1, num2))
```

## Question 1
*(Generated and modified from chatGPT)*
The web application should hash passwords before storing them in a database. Hashing converts the password into a fixed-length string of characters, which cannot be easily reversed. Additionally, adding a unique salt to each password before hashing ensures that identical passwords results in difference hashes, preventing precomputed attacks such as rainbow table attack.

**Pseudocode**:
```pseudo
STOREPASSWORD(USERNAME, PASSWORD):
    SALT = GENERATEUNIQUESTRING()
    HASHEDPASSWORD = HASH(PASSWORD + SALT)
    STORE(USERNAME, HASHEDPASSWORD, SALT)

AUTHENTICATE(USERNAME, INPUTPASSWORD):
    STOREDHASH, STOREDsalt = RETRIEVE(USERNAME)
    INPUTHASH = HASH(INPUTPASSWORD + STOREDsalt)
    IF INPUTHASH == STOREDHASH:
        GRANTACCESS()
    ELSE:
        DENYACCESS()

FUNCTION GENERATEUNIQUESTRING():
    RETURN RANDOMSTRING()

FUNCTION HASH(INPUT):
    RETURN SECUREHASHFUNCTION(INPUT)
```


# Part C
---
## Question 1
### 1.a)
**Dynamic Programming (DP)** is a method for solving complex problems by breaking them down into simpler subproblems, solving each subproblem only once, and storing their solutions to avoid recomputation. It also ensures a globally optimal solution by solving each subproblem optimally.

**Overlapping Subproblems**: A characteristic of DP where subproblems recur multiple times. Dynamic programming takes advantage of this by storing solutions to subproblems and reusing them, avoiding redundant computations.

### 1.b)
**Tabulation**
```pseudo
FUNCTION FIBONACCI(N):
	INITIALIZE NEW ARRAY A OF SIZE N+1
	IF N == 1 OR N == 2 THEN
		RETURN 1
	ENDIF
	A[1] = 1
	FOR i FROM 2 TO N DO
		A[i] = A[i-1] + A[i-2]
	ENDFOR
	RETURN A[N]
END FUNCTION
```

**Memoization**:
```pseudo
FUNCTION MEMO(TABLE, N):
	IF N == 1 OR N == 0 THEN
		RETURN N
	ENDIF
	IF TABLE[N] != -1 THEN
		return TABLE[N]
	ENDIF
	TABLE[N] = MEMO(TABLE, N-1) + MEMO(TABLE, N-2)
	RETURN TABLE[N]
END FUNCTION

FUNCTION FIBONACCI(N):
	INITIALIZE NEW ARRAY TABLE OF SIZE N+1
	FOR i FROM 0 TO N DO
		TABLE[i] = -1
	ENDFOR
	RETURN MEMO(TABLE, N)
END FUNCTION
```

**Implementation**
```python
def fib_tabulation(n):
    if n == 1 or n == 2:
        return 1
    tab = [0] * (n + 1)
    tab[1] = 1
    for i in range(2, n + 1):
        tab[i] = tab[i - 1] + tab[i - 2]
    return tab[n]

def fib_memo(n):
    def helper(n, memo):
        if memo[n] != -1:
            return memo[n]
        if n == 1 or n == 2:
            return 1
        memo[n] = helper(n - 1, memo) + helper(n - 2, memo)
        return memo[n]
    memo = [-1] * (n + 1)
    return helper(n, memo)
```

### 1.c)
Refer to the `.pdf`

### 1.d)
$O(n)$

### 1.e)

| Feature                       | Memoization (Top-Down)                                                                                   | Tabulation (Bottom-Up)                                                                                |
| ----------------------------- | -------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------- |
| **Approach**                  | Top-down, using recursion.                                                                               | Bottom-up, using iteration.                                                                           |
| **Problem Solving**           | Solves larger problems by breaking them into smaller subproblems and solving them if not already solved. | Identifies all subproblems first, then solves them to build up to the solution of the larger problem. |
| **Caching**                   | Caches results of function calls. Easier to deal with state transitions.                                 | Stores results in a table. Harder to deal with state transitions.                                     |
| **Order of Filling**          | Entries in the memoization table are filled on demand.                                                   | All entries in the table must be filled one by one.                                                   |
| **Handling Large Datasets**   | May struggle with larger datasets due to risk of stack overflow from deep recursion.                     | Can handle larger datasets better since it avoids deep recursion and uses iteration.                  |
| **Speed**                     | Might be slower due to function call overhead on the memory stack.                                       | Generally faster as it avoids recursion overhead.                                                     |
| **Space Complexity**          | Uses extra space for recursion stack.                                                                    | Uses only the space required for the table.                                                           |
| **Initialization**            | Initializes the memo table only as needed.                                                               | Initializes the table upfront.                                                                        |
| **Traceability**              | Easier to trace due to function calls.                                                                   | More straightforward but less intuitive.                                                              |
| **Implementation Complexity** | Easier to implement for straightforward recursive relationships.                                         | Might require more planning to ensure correct order of solving dependencies.                          |

## Question 2
### 2.a)
A minimum spanning tree of a connected, undirected graph is a subset of graph consisting of all $V$ vertices in the graph and exactly $|V-1|$ edges, with no cycles and its total weight is minimized.

### 2.b)
```pseudocode
MST-KRUSKAL(G, w)
A = null
FOR Vertex V in G.V
	MAKE-SET(v)
sort the edges of G.E into nondecreasing order by weight w
for each edge (u, v) in G.E, taken in nondecreasing order by weight
	if FIND-SET(u) != FIND-SET(v)
		A = A U {(u, v)}
		UNION(u, v)
return A
```

### 2.c)
Refer to the `.pdf`

### 2.d)
$O(E\ \text{log}\ V)$

### 2.e)

| Feature                          | Prim's Algorithm                                                          | Kruskal's Algorithm                                       |
| -------------------------------- | ------------------------------------------------------------------------- | --------------------------------------------------------- |
| **Approach**                     | Vertex-based, grows the MST one vertex at a time                          | Edge-based, adds edges in increasing order of weight      |
| **Data Structure**               | Priority queue (min-heap)                                                 | Union-Find data structure                                 |
| **Graph Representation**         | Adjacency matrix or adjacency list                                        | Edge list                                                 |
| **Initialization**               | Starts from an arbitrary vertex                                           | Starts with all vertices as separate trees (forest)       |
| **Edge Selection**               | Chooses the minimum weight edge from the connected vertices               | Chooses the minimum weight edge from all edges            |
| **Cycle Management**             | Not explicitly managed; grows connected component                         | Uses Union-Find to avoid cycles                           |
| **Complexity**                   | $O(V^2)$ for adjacency matrix, $O((E + V)\ log\ V)$ with a priority queue | $O(E\ log\ E)$ or $O(E\ log\ V)$, due to edge sorting     |
| **Suitable for**                 | Dense graphs                                                              | Sparse graphs                                             |
| **Implementation Complexity**    | Relatively simpler in dense graphs                                        | More complex due to cycle management                      |
| **Parallelism**                  | Difficult to parallelize                                                  | Easier to parallelize edge sorting and union operations   |
| **Memory Usage**                 | More memory for priority queue                                            | Less memory if edges can be sorted externally             |
| **Example Use Cases**            | Network design, clustering with dense connections                         | Road networks, telecommunications with sparse connections |
| **Starting Point**               | Requires a starting vertex                                                | No specific starting point, operates on global edges      |
| **Optimal for**                  | Dense graphs where adjacency list is used                                 | Sparse graphs where edge list is efficient                |
| **Intermediate Steps**           | Shows incremental growth of MST                                           | Less intuitive for incremental MST construction           |
| **Handling Disconnected Graphs** | Assumes connected graph                                                   | Can handle disconnected graphs                            |
| **Traversal**                    | May traverse a node more than once                                        | Traverses each node only once                             |
| **Edge List Preference**         | Prefers adjacency matrix/list                                             | Prefers edge list                                         |
| **Heap Data Structure**          | Utilizes heap for edge selection                                          | Does not inherently use heap                              |

1. **Approach:**
   - **Prim's Algorithm:** It is vertex-based and grows the Minimum Spanning Tree (MST) by starting from an arbitrary vertex and adding the smallest edge that connects the growing MST to a new vertex.
   - **Kruskal's Algorithm:** It is edge-based and constructs the MST by adding edges in increasing order of weight, ensuring no cycles are formed.

2. **Data Structure:**
   - **Prim's Algorithm:** Utilizes a priority queue (min-heap) to efficiently select the smallest edge connecting the growing MST to a new vertex.
   - **Kruskal's Algorithm:** Employs the Union-Find data structure to manage and merge sets of vertices while avoiding cycles.

3. **Graph Representation:**
   - **Prim's Algorithm:** Works well with adjacency matrices or adjacency lists, especially effective for dense graphs.
   - **Kruskal's Algorithm:** Best suited for edge lists, making it efficient for sparse graphs.

4. **Initialization and Edge Selection:**
   - **Prim's Algorithm:** Begins with a single arbitrary vertex and iteratively adds the smallest edge from the growing MST to a new vertex.
   - **Kruskal's Algorithm:** Starts with all vertices as separate components (a forest) and iteratively adds the smallest edge, merging the components.

5. **Complexity:**
   - **Prim's Algorithm:** Runs in $O(V^2)$ for adjacency matrices and $O(E + V \log V)$ with a priority queue.
   - **Kruskal's Algorithm:** Runs in $O(E \log E)$ or $O(E \log V)$ due to the initial sorting of edges.

6. **Handling of Graph Types:**
   - **Prim's Algorithm:** Assumes a connected graph and may traverse a node multiple times.
   - **Kruskal's Algorithm:** Can handle disconnected graphs and traverses each node only once.

In summary, Prim's algorithm is optimal for dense graphs and uses a priority queue to expand the MST one vertex at a time, whereas Kruskal's algorithm is better suited for sparse graphs, employing edge sorting and Union-Find to build the MST by adding the smallest edges without forming cycles.

# Reference
---
1. Admin. (2023, March 29). _Difference between Prim’s and Kruskal’s Algorithm for MST_. BYJUS. https://byjus.com/gate/difference-between-prims-and-kruskal-algorithum/
2. GeeksforGeeks. (2024, May 27). _Difference between Prim s and Kruskal s algorithm for MST_. GeeksforGeeks. https://www.geeksforgeeks.org/difference-between-prims-and-kruskals-algorithm-for-mst/
3. GeeksforGeeks. (2024a, February 2). _Tabulation vs Memoization_. GeeksforGeeks. https://www.geeksforgeeks.org/tabulation-vs-memoization/
4. _Tabulation vs Memoization - javatpoint_. (n.d.-b). www.javatpoint.com. https://www.javatpoint.com/tabulation-vs-memoization