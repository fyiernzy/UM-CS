# Question 1
## 1.a)
### Fixed Partitioning
Fixed partitioning divides memory into fixed (static) partitions and allocates jobs with sizes smaller than or equal to the partition size. The entire job is stored contiguously in the memory throughout its execution.

**Strength**
Fixed partition is easy to to implement because jobs are allocated to fixed partitions, saving the computation required for complex memory compaction.

**Weakness**
1. Jobs larger than a partition size will be rejected and cannot be executed. 
2. Larger jobs may experience longer turnaround times while waiting for a sufficiently large free partition.
3. Jobs smaller than a partition still occupy the entire partition, leading to internal fragmentation.
	1. Memory protection ensures that once a partition is assigned to a job, it cannot be used by another job.
	2. Internal fragmentation is the wasted space within a fixed partition due to the mismatch between the job size and partition size.

### Dynamic Partitioning
Dynamic Partitioning assigns as much memory as the job requests when it is loaded. Hence, it fully utilizes memory initially. Jobs are loaded on a a First-Come-First-Serve basis. 

**Strength**
Compared to Fixed Partitioning, dynamic partitioning allows larger jobs to be executed and prevents internal fragmentation. 

**Weakness**
However, dynamic partitioning lead to the external fragmentation, creating small, unusable memory spaces between the allocated memory blocks. This occurs when a large job is freed and smaller jobs take up the space. Dynamic partitioning also requires additional overhead for memory compaction, the process of gathering scatter free memory space to into larger, continuous memory space to accommodate other jobs.

## 1.b)
### Compaction
**Compaction** is a technique used to eliminate external fragmentation by gathering scattered small free memory blocks into a single large contiguous block. This allows larger jobs to be accommodated without being rejected due to fragmented memory spaces. 

However, compaction requires significant computational resources to rearrange memory. It involves:
- **Tracking**: Maintaining a table to keep track of every job in memory to avoid data and context loss.
- **Timing**: Determining the optimal compaction timing. Frequent compaction can cause high overhead, while infrequent compaction can lead to job congestion.
- **Mechanism**: Deciding the compaction method because the locations of free memory blocks vary.

### Fixed Partition / Demand Paging / Segmented Paging Allocation

**Fixed Partitioning** divides memory into fixed partitions. Only jobs smaller than or equal to the partition size can be loaded. This approach eliminates external fragmentation but causes internal fragmentation.

**Demand Paging** divides jobs into pages, the disk into sectors, and main memory into page frames, all of the same size. This approach eliminates external fragmentation but suffers from internal fragmentation because pages might not always match the frame size. It may experience frequent page swaps when page faults occur, reducing efficiency. It requires additional overhead to maintain the tables.

**Segmented Paging Allocation** divides segments (modules of the program with related functions) into smaller, equally-sized pages that match the frame size, thus eliminating external fragmentation. It reduces the need for frequent page swaps but still suffers from internal fragmentation. It also requires extra memory space and computational overhead to maintain the tables for its functions.

> As long as fixed partitions or page frames are used, internal fragmentation will occur. As long as variable-sized segments or partitions are used, external fragmentation will occur.


**Additional Notes**
> Segmented paging allocation combines the physical advantages of demand paging (same page and frame size) with the logical advantages of segmented allocation (reduces constant page swaps). Segmented allocation divides jobs into modules, each with related functions, reducing the need for constant page swaps. However, since it varies in size, it can cause external fragmentation.

# Question 2
## 2.a)
*(Inspired by Lecture W4, Processor Management, pg. 27, extracted and modified using chatGPT)* 
- Throughput
	- **Definition**: The number of processes completed per unit time.
	- **Impact**: Higher throughput indicates more processes being handled by the system in a given time frame.
- Response Time
	- **Definition**: The time from the submission of a request until the response is produced.
	- **Impact**: Shorter response times lead to faster processing of incoming jobs, resulting in higher user satisfaction.
- Turnaround Time
	- **Definition:** The total time taken from the submission of a process to its completion.
	- **Impact**: Shorter turnaround times lead to quicker job completion, enhancing system responsiveness.
- Waiting Time
	- **Definition**: The total time a process spends waiting in the ready queue before it gets executed.
	- **Impact**: Lower waiting time reduces delays for processes, leading to better system performance.
- CPU Utilization
	- **Definition**: The percentage of time the CPU is actively working on processing jobs.
	- **Impact**: Higher CPU utilization means more efficient use of the processor, leading to better overall system performance.

## 2.b)
### FCFS
- First-Come-First-Served (FCFS) is a non-preemptive scheduling algorithm that handles jobs according to their arrival time using a FIFO queue.
- FCFS is easy to implement and suitable for batch systems where jobs come sequentially. 
- However, it is not ideal for interactive systems due to potential long wait times.
- FCFS can cause varying average turnaround times, often leading to inefficiency. Some processes might take a long time to execute, causing delays for subsequent process.

### Round Robin
- Round Robin is a preemptive scheduling algorithm that allocates a fixed, predetermined time quantum to each job to ensure equal CPU distribution among all active processes. 
	- If a job completes within the time quantum, all allocated resources are released and the job is returned to the user.
	- If a job doesn't complete within the time quantum, it's preempted, its state is saved in its Process Control Block (PCB), and it is moved to the end of the READY queue.
- Round Robin  ensures that no process monopolizes the CPU *(take significantly long time to be executed)*. 
- However, it requires extra computation and memory to preserve the state of the preempted process in its PCB. 
- In high-demand situations, a process that is preempted earlier might wait a long time for its turn. 

|                  | FCFS                                                                                                                                             | Round Robin                                                                                                                                       |
| ---------------- | ------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------- |
| Type             | Non-preemptive                                                                                                                                   | Preemptive                                                                                                                                        |
| Suitability      | Suitable for batch systems                                                                                                                       | Suitable for interactive systems                                                                                                                  |
| Order            | Processes are handled based on their arrival time                                                                                                | Processes are given a fixed time quantum and are cycled through the READY queue                                                                   |
| Wait Time        | Suffers from the "convoy effect", where short processes are delayed by long processes.<br><br>Can cause highly variable average turnaround times | Ensures more even distribution of CPU time                                                                                                        |
| Complexity       | Easier to implement                                                                                                                              | More complex to implement due to the need for context switching and maintaining the PCB (Process Control Block)                                   |
| Additional notes |                                                                                                                                                  | Ensures no single process monopolizes the CPU<br><br>Provides more predictable response times. <br><br>Requires careful selection of time quantum |


## 2.c)
The size of the time quantum significantly affects system performance in Round Robin scheduling.
- **Large quantum** turns the algorithm into FCFS, reducing preemption benefits and leading to inefficient CPU utilization.
- **Small quantum** increases context switching, slowing down job execution and raising overhead significantly. Context switching involves saving and loading process states, requires additional computational resources and memory for Process Control Blocks (PCBs). 

## 2.d)
The turnaround time is the total time taken by a process from its submission to its completion.

**For FCFS scheduling:**
$$t_{ta} = t_{cpt} + t_{pt} - t_a$$
where 
- $t_{ta}$ is the turnaround time, 
- $t_{cpt}$ is the cumulative processing time, 
- $t_{pt}$ is the processing time of the process, 
- $t_a$ is the arrival time of the process

- **P1**, $t_{ta} = 0 + 6 - 0 = 6\text{ms}, t_{cpt}=6\text{ms}$
- **P2**, $t_{ta} = 6 + 10 - 2 = 14\text{ms}, t_{cpt}=16\text{ms}$
- **P3**, $t_{ta} = 16 + 4 - 6 = 14\text{ms}, t_{cpt}=20\text{ms}$
- **P4**, $t_{ta} = 20 + 2 - 12 = 10\text{ms}, t_{cpt}=22\text{ms}$
- **P5**, $t_{ta} =22 + 8 - 16 = 14\text{ms}, t_{cpt}=30\text{ms}$

**For Round Robin (time quantum = 4ms)**
- $t_r$ is the remaining processing time required
- $t$ is the current timestamp
- $t_{p}$ is the processing time

- P1, $t_r=\text{min}(0,6-4)=2\text{ms}, t=4\text{ms}$
- P2,  $t_r=\text{min}(0,10-4)=6\text{ms}, t=8\text{ms}$
- P3,  $t_r=\text{min}(0,4-4)=0\text{ms}, t=12\text{ms},t_{a}=12-6=6\text{ms}$
- P4,  $t_r=\text{min}(0,2-4)=0\text{ms}, t=16\text{ms},t_{a}=16-6=10\text{ms}$
- P5,  $t_r=\text{min}(0,8-4)=4\text{ms}, t=20\text{ms}$
- P1,  $t_r=\text{min}(0,2-4)=0\text{ms}, t=24\text{ms},t_{a}=24-0=24\text{ms}$
- P2,  $t_r=\text{min}(0,6-4)=2\text{ms}, t=28\text{ms}$
- P5,  $t_r=\text{min}(0,4-4)=0\text{ms}, t=32\text{ms},t_{a}=32-16=16\text{ms}$
- P2,  $t_r=\text{min}(0,2-4)=0\text{ms}, t=36\text{ms},t_{a}=36-2=34\text{ms}$

**Comparison**

|       | FCFS          | RR            |
| ----- | ------------- | ------------- |
| $P_1$ | $6\text{ms}$  | $24\text{ms}$ |
| $P_2$ | $14\text{ms}$ | $34\text{ms}$ |
| $P_3$ | $14\text{ms}$ | $6\text{ms}$  |
| $P_4$ | $10\text{ms}$ | $10\text{ms}$ |
| $P_5$ | $14\text{ms}$ | $16\text{ms}$ |


# Question 3
(*Refer to the Lecture Note, but it's useless TAT.*)

*(GeeksforGeeks, 2024)*
 - **Available** is a 1-d array indicating the number of available resources of each type.
 - **Max** is a 2-d array that defines the maximum demand of each process in a system.
 - **Allocation** is a 2-d array that defines the number of resources of each type currently allocated to each process.
 - **Need** is a 2-d array that indicates the remaining resource need of each process.

## 3.a)
```shell
Available Vector:
  15  5  9 10
-  2  0  2  1
-  0  1  1  1
-  4  1  0  2
-  1  0  0  1
-  1  1  0  0
-  1  0  1  1
---------------
   6  2  5  4
---------------   
```

## 3.b)
```shell
Calculation
|---------------|       |---------------|
| 9 | 5 | 5 | 5 |       | 2 | 0 | 2 | 1 |
| 2 | 2 | 3 | 3 |       | 0 | 1 | 1 | 1 |
| 7 | 5 | 4 | 4 |       | 4 | 1 | 0 | 2 |
| 3 | 3 | 3 | 2 |   -   | 1 | 0 | 0 | 1 |
| 5 | 2 | 2 | 1 |       | 1 | 1 | 0 | 0 |
| 4 | 4 | 4 | 4 |       | 1 | 0 | 1 | 1 | 
|---------------|       |---------------|
```

```bash
Need Matrix:
|---------------|
| 7 | 5 | 3 | 4 |
| 2 | 1 | 2 | 2 |
| 3 | 4 | 4 | 2 |
| 2 | 3 | 3 | 1 |
| 4 | 1 | 2 | 1 |
| 3 | 4 | 3 | 3 | 
|---------------|
```

## 3.c)
|       | Allocation |  Need   | Available |
| ----- | :--------: | :-----: | :-------: |
| $P_0$ |  2 0 2 1   | 7 5 3 4 |  6 2 5 4  |
| $P_1$ |  0 1 1 1   | 2 1 2 2 |           |
| $P_2$ |  4 1 0 2   | 3 4 4 2 |           |
| $P_3$ |  1 0 0 1   | 2 3 3 1 |           |
| $P_4$ |  1 1 0 0   | 4 1 2 1 |           |
| $P_5$ |  1  0 1 1  | 3 4 3 3 |           |
- 1 - $P_0$ 
	- $\text{Need}[0] > Available$ ($[7,5,3,4] > [6,2,5,4]$)
	- Safe sequence = `[]`
- 2 - $P_1$ 
	- $\text{Need}[1] \leq \text{Available}$ ($[2,1,2,2] \leq [6,2,5,4]$)
	- $\text{Available}=[0,1,1,1]+[6,2,5,4]=[6,3,6,5]$
	- Safe sequence = `[P1]`
-  3 - $P_2$ 
	- $\text{Need}[2] > \text{Available}$ ($[3,4,4,2] > [6,3,6,5]$)
	- Safe sequence = `[P1]`
- 4 - $P_3$ 
	- $\text{Need}[3] \leq \text{Available}$ ($[2,3,3,1] \leq [6,3,6,5]$)
	- $\text{Available}=[1,0,0,1]+[6,3,6,5]=[7,3,6,6]$
	- Safe sequence = `[P1,P3]`
- 5 - $P_4$ 
	- $\text{Need}[4] \leq \text{Available}$ ($[4,1,2,1] \leq [7,3,6,6]$)
	- $\text{Available}=[1,1,0,0]+[7,3,6,6]=[8,4,6,6]$
	- Safe sequence = `[P1,P3,P4]`
- 6 - $P_5$ 
	- $\text{Need}[5] \leq \text{Available}$ ($[3,4,3,3] \leq [8,4,6,6]$)
	- $\text{Available}=[1,0,1,1]+[8,4,6,6]=[9,4,7,7]$
	- Safe sequence = `[P1,P3,P4,P5]`
- 7 - $P_0$ 
	- $\text{Need}[0] > Available$ ($[7,5,3,4] > [9,4,7,7]$)
	- Safe sequence = `[P1,P3,P4,P5]`
- 8 - $P_2$ 
	- $\text{Need}[5] \leq \text{Available}$ ($[3,4,4,2] \leq [9,4,7,7]$)
	- $\text{Available}=[4,1,0,2]+[9,4,7,7]=[13,5,7,9]$
	- Safe sequence = `[P1,P3,P4,P5,P2]`
- 9 - $P_0$ 
	- $\text{Need}[0] \leq \text{Available}$ ($[7,5,3,4] \leq [9,4,7,7]$)
	- $\text{Available}=[2,0,2,1]+[13,5,7,9]=[15,5,9,10]$
	- Safe sequence = `[P1,P3,P4,P5,P2,P0]`

Yes, the current allocation state is safe since all processes can be finished.
- In a safe state, at least one job can finish because there are enough available resources to satisfy its maximum needs.
- Using the resources released by the finished job, the maximum needs of another job can be met, allowing that job to finish. This process continues until all jobs terminate successfully.

![[Pasted image 20240618154836.png]]
## 3.d)
The question is vague and missing some information:
1. What does *request of resources* mean? Does it refer to the **Claim** vector or the **Need** vector?
2. When does P5 arrive? Do we need to check a specific sequence to ensure processes terminate, or just check if the allocation is in a safe state?
3. What does it mean to *grant the request*? Does it mean P5 can finally finish, or will the request change trigger a transition from safe to unsafe?

However, regardless of whether the request refers to the Need or Claim vector, it doesn't matter. It won't trigger a transition from safe to unsafe because all resources can be finished, even with a Need vector of $<3,4,3,3>$ and a Claim vector of $<4,4,4,4>$. Therefore, all processes can finish at some point. To prove it, just recalculate all the matrices and repeat the calculation.

# Question 4
(*I didn't find the corresponding chapter in the Lecture Note, so I just copied from ChatGPT, assuming Embedded Systems will not be on the upcoming examination.*)

### Characteristics of Embedded Operating System (OS)

1. **Real-Time Performance**: Embedded OS must provide real-time performance with predictable response times to handle time-critical tasks.
2. **Resource Efficiency**: Designed to operate with limited memory and processing power.
3. **Reliability and Stability**: High reliability and stability are essential since these systems often run continuously and must handle errors gracefully.
4. **Minimal User Interface**: Typically, embedded OS have minimal or no user interface, as they are designed for specific tasks.
5. **Low Power Consumption**: Optimized for low power consumption to extend the life of battery-operated devices.

### Two Approaches in Developing Embedded OS

1. **Monolithic Kernel Approach**:
   - **Definition**: In this approach, the OS kernel is a single large block of code that runs in a single address space. All OS services like file system, memory management, and device drivers are part of the kernel.
   - **Advantages**: High performance due to the absence of context switches between kernel services; simpler inter-process communication.
   - **Disadvantages**: Larger memory footprint; harder to maintain and debug due to the complexity.

2. **Microkernel Approach**:
   - **Definition**: This approach separates the basic OS services (like communication, process management) into a small, modular kernel. Other services run in user space as separate processes.
   - **Advantages**: Better modularity and flexibility; easier to maintain and debug; more secure since less code runs in kernel mode.
   - **Disadvantages**: Potential performance overhead due to increased context switching and communication between user space and kernel space.

# Reference
---
1. GeeksforGeeks. (2024, May 9). _Banker s Algorithm in Operating System_. GeeksforGeeks. https://www.geeksforgeeks.org/bankers-algorithm-in-operating-system-2/
2. Scholarly Things. (2017, July 21). _Banker’s algorithm for deadlock avoidance | An example._ [Video]. YouTube. https://www.youtube.com/watch?v=2V2FfP_olaA
3. Computer Science. (2016, November 27). _Banker’s Algorithm explained_ [Video]. YouTube. https://www.youtube.com/watch?v=T0FXvTHcYi4
4. GeeksforGeeks. (2017, June 14). _Banker’s Algorithm | Operating Systems | GeeksforGeeks_ [Video]. YouTube. https://www.youtube.com/watch?v=lMNrmDUJ3GY